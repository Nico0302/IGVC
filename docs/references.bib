@misc{AlbumentationsFastFlexible,
  title = {Albumentations: Fast and Flexible Image Augmentations},
  shorttitle = {Albumentations},
  journal = {Albumentations},
  urldate = {2025-05-02},
  abstract = {Improve computer vision models with Albumentations, the fast and flexible Python library for high-performance image augmentation. Supports images, masks, bounding boxes, keypoints \& easy framework integration.},
  howpublished = {https://albumentations.ai/},
  langid = {english}
}

@misc{chandhokUNetTrainingImage2021,
  title = {U-{{Net}}: {{Training Image Segmentation Models}} in {{PyTorch}}},
  shorttitle = {{{UNet Tutorial}}},
  author = {Chandhok, Shivam},
  year = {2021},
  month = nov,
  journal = {PyImageSearch},
  urldate = {2025-04-21},
  abstract = {U-Net: Learn to use PyTorch to train a deep learning image segmentation model. We'll use Python PyTorch, and this post is perfect for someone new to PyTorch.},
  langid = {american}
}

@misc{Datasets,
  title = {Datasets},
  urldate = {2025-05-01},
  abstract = {We're on a journey to advance and democratize artificial intelligence through open source and open science.},
  howpublished = {https://huggingface.co/docs/datasets/en/index}
}

@book{HML3,
  title = {Hands-{{On Machine Learning}} with {{Scikit-Learn}}, {{Keras}}, and {{TensorFlow}}},
  shorttitle = {{{HML3}}},
  author = {G{\'e}ron, Aur{\'e}lien},
  year = {2022},
  edition = {3rd ed},
  publisher = {O'Reilly Media, Inc.},
  isbn = {978-1-0981-2596-7},
  langid = {english},
  annotation = {OCLC: 1395691153\\
Pinned\_Collections: JKX2EXGG},
  file = {/Users/nicolas/Zotero/storage/AAYDF6UW/GÃ©ron - 2022 - Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow.pdf}
}

@misc{IGVCOfficialCompetition2024,
  title = {{{IGVC Official Competition Details}}, {{Rules}} and {{Format}}},
  shorttitle = {{{IGVC Rules}}},
  year = {2024},
  month = jul,
  publisher = {Oakland University},
  file = {/Users/nicolas/Zotero/storage/YUL8R7VL/2025rules.pdf}
}

@article{kirillovUnifiedArchitectureInstance,
  title = {A {{Unified Architecture}} for {{Instance}} and {{Semantic Segmentation}}},
  author = {Kirillov, Alexander and He, Kaiming and Girshick, Ross and Doll{\'a}r, Piotr},
  langid = {english},
  file = {/Users/nicolas/Zotero/storage/DHGGWDMS/Kirillov et al. - A Unified Architecture for Instance and Semantic Segmentation.pdf}
}

@misc{linFeaturePyramidNetworks2017,
  title = {Feature {{Pyramid Networks}} for {{Object Detection}}},
  author = {Lin, Tsung-Yi and Doll{\'a}r, Piotr and Girshick, Ross and He, Kaiming and Hariharan, Bharath and Belongie, Serge},
  year = {2017},
  month = apr,
  number = {arXiv:1612.03144},
  eprint = {1612.03144},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1612.03144},
  urldate = {2025-05-05},
  abstract = {Feature pyramids are a basic component in recognition systems for detecting objects at different scales. But recent deep learning object detectors have avoided pyramid representations, in part because they are compute and memory intensive. In this paper, we exploit the inherent multi-scale, pyramidal hierarchy of deep convolutional networks to construct feature pyramids with marginal extra cost. A top-down architecture with lateral connections is developed for building high-level semantic feature maps at all scales. This architecture, called a Feature Pyramid Network (FPN), shows significant improvement as a generic feature extractor in several applications. Using FPN in a basic Faster R-CNN system, our method achieves state-of-the-art single-model results on the COCO detection benchmark without bells and whistles, surpassing all existing single-model entries including those from the COCO 2016 challenge winners. In addition, our method can run at 5 FPS on a GPU and thus is a practical and accurate solution to multi-scale object detection. Code will be made publicly available.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/nicolas/Zotero/storage/6P6N8428/Lin et al. - 2017 - Feature Pyramid Networks for Object Detection.pdf}
}

@misc{OsuigvcJoyriderosmain2024,
  title = {Osu-Igvc/Joyride-Ros-Main},
  year = {2024},
  month = may,
  urldate = {2025-02-20},
  abstract = {Full autonomy stack for Joyride, an autonomous car for Oklahoma State University.},
  howpublished = {osu-igvc}
}

@misc{PolishedScratchedPlastic,
  title = {Polished and {{Scratched Plastic}}},
  urldate = {2025-04-22},
  abstract = {It is a polished plastic, so brush marks appear on the surface. There are also tremendous tiny scratches on the surface.It is an excellent texture for plastic 3D models such as old toys.The color of the plastic and scratches can be changed by the Adobe Substance Designer archive file. More importantly, the length, width and amount of scratches can be adjusted.},
  howpublished = {https://www.texturecan.com/details/372/},
  langid = {english}
}

@misc{QubvelorgSegmentation_modelspytorch2025,
  title = {Qubvel-Org/Segmentation\_models.Pytorch},
  shorttitle = {Smp},
  year = {2025},
  month = may,
  urldate = {2025-05-02},
  abstract = {Semantic segmentation models with 500+ pretrained convolutional and transformer-based backbones.},
  copyright = {MIT},
  howpublished = {qubvel-org},
  keywords = {computer-vision,deeplab-v3-plus,deeplabv3,dpt,fpn,image-processing,image-segmentation,imagenet,models,pretrained-weights,pspnet,pytorch,segformer,segmentation,segmentation-models,semantic-segmentation,transformers,unet,unet-pytorch,unetplusplus}
}

@misc{ronnebergerUNetConvolutionalNetworks2015,
  title = {U-{{Net}}: {{Convolutional Networks}} for {{Biomedical Image Segmentation}}},
  shorttitle = {U-{{Net}}},
  author = {Ronneberger, Olaf and Fischer, Philipp and Brox, Thomas},
  year = {2015},
  month = may,
  number = {arXiv:1505.04597},
  eprint = {1505.04597},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1505.04597},
  urldate = {2025-05-05},
  abstract = {There is large consent that successful training of deep networks requires many thousand annotated training samples. In this paper, we present a network and training strategy that relies on the strong use of data augmentation to use the available annotated samples more efficiently. The architecture consists of a contracting path to capture context and a symmetric expanding path that enables precise localization. We show that such a network can be trained end-to-end from very few images and outperforms the prior best method (a sliding-window convolutional network) on the ISBI challenge for segmentation of neuronal structures in electron microscopic stacks. Using the same network trained on transmitted light microscopy images (phase contrast and DIC) we won the ISBI cell tracking challenge 2015 in these categories by a large margin. Moreover, the network is fast. Segmentation of a 512x512 image takes less than a second on a recent GPU. The full implementation (based on Caffe) and the trained networks are available at http://lmb.informatik.uni-freiburg.de/people/ronneber/u-net .},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/nicolas/Zotero/storage/TAI5FWWT/Ronneberger et al. - 2015 - U-Net Convolutional Networks for Biomedical Image Segmentation.pdf}
}

@misc{rouliiiieUsingDiffusionModels2024,
  title = {{Using diffusion models to generate synthetic data for real-life projects}},
  author = {{rouliiiie}},
  year = {2024},
  month = feb,
  journal = {Ridge-institute R\&D Blog},
  urldate = {2025-02-20},
  abstract = {Hello, this is Aur{\'e}lie, working as an Artificial Intelligence Engineer at Ridge-i. Today, I would like to introduce a way to use synthetic data generated with diffusion models to improve the performance of our models even when working with difficult data (low amount of data, unusual data) ! Disclaim{\dots}},
  howpublished = {https://iblog.ridge-i.com/entry/2024/02/22/130409},
  langid = {japanese}
}

@misc{TrainingPyTorchPyTorch,
  title = {Training with {{PyTorch}} --- {{PyTorch Tutorials}} 2.7.0+cu126 Documentation},
  urldate = {2025-05-01},
  howpublished = {https://pytorch.org/tutorials/beginner/introyt/trainingyt.html},
  file = {/Users/nicolas/Zotero/storage/2JICVKZJ/trainingyt.html}
}

@misc{zemlinIGVCDesignReport2024,
  title = {{{IGVC Design Report}} "{{Danger Zone}}"},
  shorttitle = {Danger {{Zone}}},
  author = {Zemlin, Dylan and Brown, Daniel and Chappell, Antonio and Roland, Abby and Easley, Vivian and Zhang, Yuhao and Falcone, Michael and Gonzales, Matthew and Kahn, Brandon and Kang, Min},
  year = {2024},
  month = may,
  file = {/Users/nicolas/Zotero/storage/97KNUNX5/17.pdf}
}
